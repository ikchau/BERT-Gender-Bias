{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow==1.15.0\n",
    "# !pip install -q tensorflow-hub\n",
    "# !pip install -q seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /home/ikchau/anaconda3/lib/python3.7/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: six in /home/ikchau/anaconda3/lib/python3.7/site-packages (from bert-tensorflow) (1.12.0)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  1.15.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.7.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import strings\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# !pip install -q tensorflow-hub\n",
    "# !pip install -q tfds-nightly\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import time\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ikchau/anaconda3/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import bert\n",
    "# from bert import run_classifier\n",
    "# from bert import optimization\n",
    "# from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up notebook to work with our large dataset\n",
    "import resource\n",
    "low, high = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (2**14, 2**14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d0528da17d55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# test_ex, test_HL = tfds.as_numpy(test_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training entries: {}, validation entries: {}, test entries: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'val_data' is not defined"
     ]
    }
   ],
   "source": [
    "# download, split dataset using TFDS\n",
    "# we receive the data inside of tensor objects\n",
    "\n",
    "train_data, validation_data, test_data = tfds.load(name=\"cnn_dailymail\", \n",
    "                                                   split=[\"train\", \"validation\", \"test\"], \n",
    "                                                   batch_size=-1, \n",
    "                                                   as_supervised=True)\n",
    "\n",
    "# #separate into articles and highlights (ex and HL)\n",
    "# train_ex, train_HL = tfds.as_numpy(train_data)\n",
    "# val_ex, val_HL = tfds.as_numpy(validation_data)\n",
    "# test_ex, test_HL = tfds.as_numpy(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more than 300,000 articles in total. We probably won't use that many, but here we are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 287113, validation entries: 13368, test entries: 11490\n"
     ]
    }
   ],
   "source": [
    "print(\"Training entries: {}, validation entries: {}, test entries: {}\".format(len(train_data[0]), len(validation_data[0]), len(test_data[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_train = list(train_data[:5000])\n",
    "# mini_val = list(validation_data[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mini training/validation datasets for development\n",
    "# possibly unnecessary for the way we're implementing BERT.\n",
    "# mini_train_ex, mini_train_HL = train_ex[:5000], train_HL[:5000]\n",
    "# mini_val_ex, mini_val_HL = val_ex[:5000], val_HL[:5000]\n",
    "# mini_train_ex[:1], mini_train_HL[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use .numpy to get data from tensor object, and stringify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\\\\nHe contracted the infection through contaminated food in Italy .\\\\nChurch members in Fargo, Grand Forks and Jamestown could have been exposed .'\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first index chooses highlights or summary, while second index is example number\n",
    "# sliced_train_articles[0][1] gives article with index 1\n",
    "# .numpy extracts the tensor value (in bytes)\n",
    "# we stringify it before adding to pandas\n",
    "\n",
    "str(train_data[1].numpy()[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Pandas tables and CSVs from our data.\n",
    "After a few unsuccessful attempts to extract the data from the training set, we discovered that batching the process sped it up enormously. Originally, it took about 1 to 10 seconds to process a single example from the training set. Slicing the training data into sets of 5000 examples allowed us to finish processing the set in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#287,113 training examples divided into slices of 5000 is 57.4 slices\n",
    "slices = list(range(0,58))\n",
    "\n",
    "sliced_train_articles = []\n",
    "sliced_train_highlights = []\n",
    "\n",
    "#divide training data into list of lists w/ 5000 tensors/examples each\n",
    "for ii in slices:\n",
    "    start = ii*5000\n",
    "    end = (ii+1)*5000\n",
    "    sliced_train_articles.append(list(train_data[0][start:end]))\n",
    "    sliced_train_highlights.append(list(train_data[1][start:end]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 9.266492366790771 seconds ---\n"
     ]
    }
   ],
   "source": [
    "train_articles = []\n",
    "train_highlights = []\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "#loop through list of lists\n",
    "for ii in range(0,58):\n",
    "    #special case for last list with <5000 examples\n",
    "    if ii == 57:\n",
    "        for jj in range(0,2113):\n",
    "            #append stringified value into corresponding list, article or highlight\n",
    "            train_articles.append(str(sliced_train_articles[ii][jj].numpy()))\n",
    "            train_highlights.append(str(sliced_train_highlights[ii][jj].numpy()))   \n",
    "    else:\n",
    "        for jj in range(0,5000):\n",
    "            train_articles.append(str(sliced_train_articles[ii][jj].numpy()))\n",
    "            train_highlights.append(str(sliced_train_highlights[ii][jj].numpy()))   \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put our data into a Pandas DataFrame for ease of use with different models and examples floating around the internet as notebooks and blog posts. Exporting it as CSV allows us to easily import the data into any project. The data saved as CSV is negligibly larger than when it is stored in other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip into list of tuples, prep for pandas\n",
    "cnn_train = list(zip(train_articles, train_highlights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to dataframe\n",
    "df_train = pd.DataFrame(data=cnn_train, dtype = 'str', columns=['article','highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as csv for posterity\n",
    "with open('train_dataframe.csv', 'w') as f1:\n",
    "    df_train.to_csv(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the process with validation and test data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.42009592056274414 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#13,368 validation examples divided into slices of 5000 is 3 slices\n",
    "slices = list(range(0,3))\n",
    "\n",
    "sliced_val_articles = []\n",
    "sliced_val_highlights = []\n",
    "\n",
    "#divide training data into list of lists w/ 5000 tensors/examples each\n",
    "for ii in slices:\n",
    "    start = ii*5000\n",
    "    end = (ii+1)*5000\n",
    "    sliced_val_articles.append(list(validation_data[0][start:end]))\n",
    "    sliced_val_highlights.append(list(validation_data[1][start:end]))\n",
    "    \n",
    "    \n",
    "val_articles = []\n",
    "val_highlights = []\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "#loop through list of lists\n",
    "for ii in range(0,3):\n",
    "    #special case for last list with <5000 examples\n",
    "    if ii == 2:\n",
    "        for jj in range(0,3367):\n",
    "            #append stringified value into corresponding list, article or highlight\n",
    "            val_articles.append(str(sliced_val_articles[ii][jj].numpy()))\n",
    "            val_highlights.append(str(sliced_val_highlights[ii][jj].numpy()))   \n",
    "    else:\n",
    "        for jj in range(0,5000):\n",
    "            val_articles.append(str(sliced_val_articles[ii][jj].numpy()))\n",
    "            val_highlights.append(str(sliced_val_highlights[ii][jj].numpy()))   \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))    \n",
    "\n",
    "#zip into list of tuples, prep for pandas\n",
    "cnn_val = list(zip(val_articles, val_highlights))\n",
    "\n",
    "#to dataframe\n",
    "df_val = pd.DataFrame(data=cnn_val, dtype = 'str', columns=['article','highlights'])\n",
    "\n",
    "#save as csv for posterity\n",
    "with open('validation_dataframe.csv', 'w') as f1:\n",
    "    df_val.to_csv(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.35894203186035156 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#11,490 test examples divided into slices of 5000 is 3 slices\n",
    "slices = list(range(0,3))\n",
    "\n",
    "sliced_test_articles = []\n",
    "sliced_test_highlights = []\n",
    "\n",
    "#divide training data into list of lists w/ 5000 tensors/examples each\n",
    "for ii in slices:\n",
    "    start = ii*5000\n",
    "    end = (ii+1)*5000\n",
    "    sliced_test_articles.append(list(test_data[0][start:end]))\n",
    "    sliced_test_highlights.append(list(test_data[1][start:end]))\n",
    "    \n",
    "    \n",
    "test_articles = []\n",
    "test_highlights = []\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "#loop through list of lists\n",
    "for ii in range(0,3):\n",
    "    #special case for last list with <5000 examples\n",
    "    if ii == 2:\n",
    "        for jj in range(0,1489):\n",
    "            #append stringified value into corresponding list, article or highlight\n",
    "            test_articles.append(str(sliced_test_articles[ii][jj].numpy()))\n",
    "            test_highlights.append(str(sliced_test_highlights[ii][jj].numpy()))   \n",
    "    else:\n",
    "        for jj in range(0,5000):\n",
    "            test_articles.append(str(sliced_test_articles[ii][jj].numpy()))\n",
    "            test_highlights.append(str(sliced_test_highlights[ii][jj].numpy()))   \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))    \n",
    "\n",
    "#zip into list of tuples, prep for pandas\n",
    "cnn_test = list(zip(test_articles, test_highlights))\n",
    "\n",
    "#to dataframe\n",
    "df_test = pd.DataFrame(data=cnn_test, dtype = 'str', columns=['article','highlights'])\n",
    "\n",
    "#save as csv for posterity\n",
    "with open('test_dataframe.csv', 'w') as f1:\n",
    "    df_test.to_csv(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
